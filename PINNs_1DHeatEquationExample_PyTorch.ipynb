{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2b8b36",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    <h1> Tutorial 2 </h1> \n",
    "    <h2> Physics-Informed Neural Networks Part 2</h2>\n",
    "    <h2> 1D Heat Equation PINNs Example </h2>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db24481",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook is based on two papers: *[Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations](https://www.sciencedirect.com/science/article/pii/S0021999118307125)* and *[Hidden Physics Models: Machine Learning of Nonlinear Partial Differential Equations](https://www.sciencedirect.com/science/article/pii/S0021999117309014)* with the help of  Fergus Shone and Michael Macraild.\n",
    "\n",
    "These tutorials will go through solving Partial Differential Equations using Physics-Informed Neural Networks, focusing on the Burgers Equation and a more complex example using the Navier Stokes Equation.\n",
    "\n",
    "**This introduction section is replicated in all PINN tutorial notebooks (please skip if you've already been through).** \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd71b6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "<h1>Physics-Informed Neural Networks</h1>\n",
    "\n",
    "For a typical neural network using algorithms like gradient descent to look for a hypothesis, the data are the only guide. However, if the data are noisy or sparse, and we already have governing physical models, we can use the knowledge we already have to optimise and inform the algorithms. This can be done via [feature engineering](https://www.ibm.com/think/topics/feature-engineering) or by adding a physical inconsistency term to the loss function.\n",
    "\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/physics-guided-neural-networks-pgnns-8fe9dbad9414\">\n",
    "<img src=\"https://miro.medium.com/max/700/1*uM2Qh4PFQLWLLI_KHbgaVw.png\">\n",
    "</a>   \n",
    " \n",
    " \n",
    "## The very basics\n",
    "\n",
    "If you are new to neural networks, there is a [toy neural network Python code example](https://github.com/cemac/LIFD_ENV_ML_NOTEBOOKS/tree/main/ToyNeuralNetwork) included in the [LIFD ENV ML Notebooks Repository](https://github.com/cemac/LIFD_ENV_ML_NOTEBOOKS). There we cover some of the fundamentals of neural nets and show how to build a two-layer neural network from scatch.\n",
    "\n",
    "    \n",
    "## Recommended reading\n",
    "    \n",
    "The in-depth theory behind neural networks will not be covered here as this tutorial focuses on the application of machine-learning methods. If you wish to learn more, here are some great starting points:\n",
    " \n",
    "\n",
    "* [All you need to know on neural networks](https://towardsdatascience.com/nns-aynk-c34efe37f15a) \n",
    "* [Introduction to neural networks](https://victorzhou.com/blog/intro-to-neural-networks/)\n",
    "* [Physics-Guided Neural Networks](https://towardsdatascience.com/physics-guided-neural-networks-pgnns-8fe9dbad9414)\n",
    "* [Maziar Rassi's Physics-Informed Neural Networks GitHub page](https://maziarraissi.github.io/PINNs/)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f224fe2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "<h1> Machine Learning Theory </h1>\n",
    "<a href=\"https://victorzhou.com/series/neural-networks-from-scratch/\">\n",
    "<img src=\"https://victorzhou.com/media/nn-series/network.svg\">\n",
    "</a>\n",
    "\n",
    "    \n",
    "## Physics-Informed Neural Networks\n",
    "\n",
    "Neural networks work by using lots of data to tune weights and biases, thereby minimising the loss function and enabling them to act as universal function approximators. However, these purely data-driven models lose their robustness when data is limited. By using known physical laws or empirically validated relationships, the solutions from neural networks can be sufficiently constrained by disregarding unrealistic solutions.\n",
    "    \n",
    "A Physics-Informed Neural Network considers a parameterised and non-linear partial differential equation in the general form:\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\\begin{align}\n",
    "     u_t + \\mathcal{N}[u; \\lambda] &= 0, && x \\in \\Omega, t \\in [0,T],\\\\\n",
    "\\end{align}\n",
    "    \n",
    "\n",
    "\n",
    "where $\\mathcal{u(t,x)}$ denotes the latent solution, $\\mathcal{N}$ is a non-linear differential operator acting on $u$, $\\mathcal{\\lambda}$ and $\\Omega$ is a subset of $\\mathbb{R}^D$ (the prescribed domain). This setup encapsulates a wide range of problems, such as diffusion processes, conservation laws,  advection-diffusion-reaction  systems,  and  kinetic  equations.\n",
    "\n",
    "Here we will go though this for the 1D Heat equation and Navier stokes equations.\n",
    "\n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e8586e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "<h1> Python </h1>\n",
    "\n",
    "## PyTorch\n",
    "    \n",
    "There are many machine-learning libraries available for Python. [PyTorch](https://pytorch.org/) a is one such library. If you have a GPU on the machine you are using, PyTorch should automatically use it and run the code in the notebooks even faster! This will work automatically with google Colab. If using your own machine, please ensure that the GPU-enabled version of PyTorch is installed.\n",
    "\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "* [Running Jupyter Notebooks](https://jupyter.readthedocs.io/en/latest/running.html#running)\n",
    "* [PyTorch optimisers](https://nbviewer.org/github/bentrevett/a-tour-of-pytorch-optimizers/blob/main/a-tour-of-pytorch-optimizers.ipynb)\n",
    "\n",
    "\n",
    "</div>\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc348739",
   "metadata": {},
   "source": [
    "<div style=\"background-color:  #f4b85d; padding: 10px;\">\n",
    "    \n",
    "<h1> Requirements </h1>\n",
    "\n",
    "These notebooks should run with the following requirements satisfied.\n",
    "\n",
    "<h2> Python Packages: </h2>\n",
    "\n",
    "* Python 3\n",
    "* PyTorch\n",
    "* NumPy \n",
    "* Matplotlib\n",
    "* SciPy\n",
    "\n",
    "<h2> Data Requirements</h2>\n",
    "    \n",
    "This notebook refers to some data included in the GitHub repository that was imported via the `git submodules` command mentioned in the installation instructions.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9738e",
   "metadata": {},
   "source": [
    "**Contents:**\n",
    "\n",
    "1. [1D Heat Equation non-ML Example](PINNs_1DHeatEquations_nonML.ipynb)\n",
    "2. **[1D Heat Equation PINN Example](PINNs_1DEquationExample.ipynb)**\n",
    "    * [1D Heat Equation Forwards](#1D-Heat-Equation-Forwards)\n",
    "    * [1D Heat Equation Inverse](#1D-Heat-Equation-Inverse)\n",
    "3. [Navier-Stokes PINNs Discovery of PDEs](PINNs_NavierStokes_example.ipynb)\n",
    "4. [Navier-Stokes PINNs Hidden Fluid Mechanics](PINNs_NavierStokes_HFM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab403585",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "Load in all required modules (including some auxiliary code) and turn off warnings.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ac7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For readability: disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a356009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "from itertools import product, combinations\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as la\n",
    "from pyDOE import lhs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109fdc1b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Solving the 1D Heat Equation via Neural Networks\n",
    "\n",
    "# 1D Heat Equation Forwards\n",
    "\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "**Model Problem: 1D Heat Equation**\n",
    "\n",
    "We begin by describing the first model problem - the one-dimensional heat equation. \n",
    "\n",
    "The heat equation is the prototypical parabolic partial differential equation and can be applied to modelling the diffusion of heat through a given region, hence its name. Read more about the heat equation here: https://en.wikipedia.org/wiki/Heat_equation.\n",
    "\n",
    "In 1D, the heat equation can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial u}{\\partial t} = k \\frac{\\partial^2 u}{\\partial x^2 },\n",
    "\\end{equation}\n",
    "    \n",
    "where $k$ is a material parameter called the coefficient of thermal diffusivity.\n",
    "\n",
    "This equation can be solved using numerical methods, such as finite differences or finite elements. For this notebook, we have solved the above equation numerically on a domain of $x \\in [0,1]$ and $t \\in [0, 0.25]$. Solving this equation numerically gives us a spatiotemporal domain $(x,t)$ and corresponding values of the solution $u$.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca28495c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "\n",
    "    \n",
    "Here we will describe the architecture of the PINN we use to solve the 1D heat equation in this notebook. \n",
    "![PINNS.png](PINNS.png)\n",
    "    \n",
    "    \n",
    "Net U in the above diagram approximations a function that maps from $(x,t) \\mapsto u$. $\\sigma$ represents the biases and weights for the each neuron of the network. These $\\sigma$ values are the network parameters that are updated after each iteration. AD means Automatic Differentiation - this is the chain rule-based differentiation procedure that allows for differentiation of network outputs with respect to its inputs, e.g. differentiating $u$ with respect to $x$, or calculating $\\frac{\\partial u}{\\partial x}$. The I node in the AD section represents the identity operation, i.e. keeping $u$ fixed without applying any differentiation.\n",
    "\n",
    "After the automatic differentiation part of the network, we have two separate loss function components - the data loss and the PDE loss. The data loss term is calculated by finding the difference between the network outputs/predictions $u$ and the ground truth values of $u$, which could come from simulation or experiment. The data loss term enforces the network outputs to match known data points, which are represented by the pink box labelled \"Data\". The PDE loss term is where we add the \"physics-informed\" part of the network. Using automatic differentiation, we are able to calculate derivatives of our network outputs, and so we are able to construct a loss function that forces the network to match the PDE that is known to govern the system. In this case, the PDE loss term is defined as:\n",
    " \n",
    "\\begin{equation}\n",
    "f = \\frac{\\partial u}{\\partial t} - k \\frac{\\partial^2 u}{\\partial x^2 },\n",
    "\\end{equation}\n",
    "    \n",
    "where $f$ is the residual of the 1D heat equation. By demanding that $f$ is minimised as our network trains, we ensure that the network outputs obey the underlying PDE that governs the system. We then calculate the total loss of the system as a sum of the data loss and the PDE loss.\n",
    "\n",
    "The loss is calculated after each pass through the network and, when it is above a certain tolerance, the weights and biases are updated using a gradient descent step. When the loss falls below the tolerance the network is trained. In inference mode, we can then input a fine mesh of spatiotemporal coordinates and the network will find the solution at each of these points.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082afce2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "###  Variable definitions\n",
    "\n",
    "Variables to be defined here:\n",
    "\n",
    "`X_u`: Input coordinates, e.g. spatial and temporal coordinates.\n",
    "\n",
    "`u`: Output corresponding to each input coordinate. \n",
    "\n",
    "`X_f`: Collocation points at which the governing equations are satisfied. These coordinates will have the same format as the X_u coordinates, e.g. $(x,t)$.\n",
    "\n",
    "`layers`: Specifies the structure of the U network.\n",
    "\n",
    "`lb`: Vector containing the lower bound of all of the coordinate variables, e.g. $x_{min}$, $t_{min}$.\n",
    "\n",
    "`ub`: Vector containing the upper bound of all of the coordinate variables, e.g. $x_{max}$, $t_{max}$.\n",
    "\n",
    "`k`: This is the constant material parameter for this specific problem. For this problem, the heat equation, $k$ represents thermal diffusivity.\n",
    "    \n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fbd70",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "# Load data and set input parameters \n",
    "      \n",
    "A feedforward neural network of the following structure is assumed:\n",
    "- the input is scaled elementwise to lie in the interval $[-1, 1]$,\n",
    "- followed by 8 fully connected layers each containing 20 neurons and each followed by a hyperbolic tangent activation function,\n",
    "- one fully connected output layer.\n",
    "\n",
    "This setting results in a network with a first hidden layer: $2 \\cdot 20 + 20 = 60$; $9$ intermediate layers: each $20 \\cdot 20 + 20 = 540$; output layer: $20 \\cdot 1 + 1 = 21$).\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf4360e",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "    \n",
    "# Number of collocation points \n",
    "    \n",
    "`2000` collocation points is the default setting for this example and can be increased to improve results at cost of computational speed. The original work set this to `N_u=10000`, running on GPUs in a few minutes. \n",
    "    \n",
    "    \n",
    "The network takes in data in coordinate pairs: $(x,t) \\mapsto u$.     \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004161e6",
   "metadata": {},
   "source": [
    "# Initialisation forwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c3737d",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #ffffcc; padding: 10px;\">\n",
    "\n",
    "Once you have run through the notebook once you may wish to alter any of the following: \n",
    "    \n",
    "- number of data training points `N_u`,\n",
    "- number of collocation training points `N_f`,\n",
    "- number of layers in the network `layers`,\n",
    "- number of neurons per layer `layers`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb011bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "N_u = 100 # number of data points (default 100)\n",
    "N_f = 2000 # collocation points (default 2000)\n",
    "# structure of network: two inputs (x,t) and one output u\n",
    "# 8 fully connected layers with 20 nodes per layer\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e702a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(\"Data/heatEquation_data.mat\") # these data are from the PINNs paper\n",
    "t = data['t'].flatten()[:,None] # read in t and flatten into column vector\n",
    "x = data['x'].flatten()[:,None] # read in x and flatten into column vector\n",
    " # Exact represents the exact solution to the problem, from the data provided\n",
    "Exact = np.real(data['usol']).T # Exact has structure of nx times nt\n",
    "\n",
    "print(\"usol shape = \", Exact.shape)\n",
    "\n",
    "# We need to find all the x,t coordinate pairs in the domain\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "# Flatten the coordinate grid into pairs of x,t coordinates\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) # coordinates x,t\n",
    "u_star = Exact.flatten()[:,None]   # corresponding solution value with each coordinate            \n",
    "\n",
    "print(\"X has shape \", X.shape, \", X_star has shape \", X_star.shape)\n",
    "    \n",
    "# Domain bounds (-1,1)\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)  \n",
    "\n",
    "print(\"Lower bounds of x,t: \", lb)\n",
    "print(\"Upper bounds of x,t: \", ub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee085c0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "# Select training data\n",
    "    \n",
    "The training data `X_u_train` and `u_train` are generated to include training coordinates on the boundaries. The sampling points are plotted below.\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train using internal points\n",
    "X_u_train = X_star\n",
    "u_train = u_star\n",
    "\n",
    "## Generate collocation points using Latin Hypercube sampling within the bounds of the spatiotemporal coordinates\n",
    "# Generate N_f x,t coordinates within range of upper and lower bounds\n",
    "X_f_train = lb + (ub-lb)*lhs(2, N_f) # the 2 denotes the number of coordinates we have - x,t \n",
    "\n",
    "## In addition, we add the X_u_train coordinates from the boundaries to the X_f coordinate set\n",
    "X_f_train = np.vstack((X_f_train, X_u_train)) # stack up all training x,t coordinates for u and f \n",
    "\n",
    "## We downsample the boundary data to leave N_u randomly distributed points\n",
    "## This makes the training more difficult - \n",
    "## if we used all the points then there is not much for the network to do!\n",
    "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "X_u_train = X_star[idx,:]\n",
    "u_train = u_star[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a plot to show the distribution of training data\n",
    "plt.scatter(X_f_train[:,1], X_f_train[:,0], marker='x', color='red',alpha=0.1)\n",
    "plt.scatter(X_u_train[:,1], X_u_train[:,0], marker='x', color='black')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.title('Data points and collocation points (red crosses)')\n",
    "plt.legend(['Collocation Points', 'Data Points'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3575fe",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "**$u(x,t)$** can then be defined below as the function `net_u` and the physics informed neural network **$f(x,t)$** is outlined in function `net_f`.\n",
    "\n",
    "\n",
    "`net_u()` constructs a network that takes input $x,t$ and outputs the solution $u(x,t)$.\n",
    "    \n",
    "`net_f()`  is the $f$ network is where the PDE is encoded.\n",
    "    \n",
    "1. We read in the value of $k$ first so that it can be included in the equations.\n",
    "2. Then we evaluate $u$ for the $X_f$ input coordinates (collocation points).\n",
    "3. Then we use PyTorch differentiation (autograd) to calculate the derivatives of the solution.\n",
    "4. Finally, we encode the PDE in residual form as $f->0, u_t = k*u_xx, which is the governing eq.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b472294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_u(x, t, model):\n",
    "    X = torch.cat([x, t], dim=1)\n",
    "    u = model(X)\n",
    "    return u\n",
    "\n",
    "def net_f(x, t, model, k):\n",
    "    x.requires_grad_(True)\n",
    "    t.requires_grad_(True)\n",
    "    u = net_u(x, t, model)\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    f = k * u_xx\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073426c2",
   "metadata": {},
   "source": [
    "# PyTorch module class and initialization\n",
    "\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "This code sets up a Physics-Informed Neural Network (PINN) for solving the 1D heat equation using PyTorch. Here's a summary of the key components:\n",
    "\n",
    "1. `XavierInit` Class:\n",
    "\n",
    "Custom layer initialization using Xavier initialization, which is designed to keep the scale of the gradients roughly the same in all layers. Initializes weights and biases for a layer.\n",
    "\n",
    "2. `initialize_NN` Function:\n",
    "\n",
    "Creates a list of layers using the XavierInit class. Takes a list of layer sizes and initializes each layer accordingly.\n",
    "\n",
    "3. `NeuralNet()` constructs the network U where X is a matrix containing the input and output coordinates, i.e. x,t,u\n",
    "and X is normalised so that all values lie between -1 and 1, this improves training. Applies the layers sequentially with the tanh activation function, except for the last layer.\n",
    "\n",
    "Using the PyTorch module classes allows you to create more complex models controlling exactly how the data flows through the model [overview of PyTorch Modules here](https://www.learnpytorch.io/02_pytorch_classification/).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInit(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(XavierInit, self).__init__()\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = torch.sqrt(torch.tensor(2.0 / (in_dim + out_dim)))\n",
    "        self.weight = nn.Parameter(torch.randn(in_dim, out_dim) * xavier_stddev)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weight) + self.bias\n",
    "\n",
    "def initialize_NN(layers):\n",
    "    weights = nn.ModuleList()\n",
    "    num_layers = len(layers)\n",
    "    for l in range(num_layers - 1):\n",
    "        layer = XavierInit(size=[layers[l], layers[l + 1]])\n",
    "        weights.append(layer)\n",
    "    return weights\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, layers, lb, ub):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.weights = initialize_NN(layers)\n",
    "        self.lb = torch.tensor(lb)\n",
    "        self.ub = torch.tensor(ub)\n",
    "\n",
    "    def forward(self, X):\n",
    "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0\n",
    "        for l in range(len(self.weights) - 1):\n",
    "            H = torch.tanh(self.weights[l](H.float()))\n",
    "        Y = self.weights[-1](H)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7dc42",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "# Initalize the neural network \n",
    "    \n",
    "Calling our PyTorch model passing in information about the neural network layers (`layers`) and bounds (`lb`, `ub`) initializes our model.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "model = NeuralNet(layers, lb, ub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416fa291",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "**Training might take a while depending on value of Train_iterations**\n",
    "\n",
    "If you set Train_iterations too low the end results will be poor. 50000 was used to achieve excellent results. \n",
    "\n",
    "* If you are using a machine with GPUs please set `Train_iterations` to 50000 and this will run quickly.\n",
    "* If you are using a well spec'ed laptop/computer, you can leave this setting `Train_iterations=50000` but it will take up to 10 mins.\n",
    "* If you are using a low spec'ed laptop/computer or cannot leave the code running, `Train_iterations=20000` is the recommended value (this solution may not be accurate).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98391e71",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00beade",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffcc; padding: 10px;\">\n",
    "\n",
    "# Advanced \n",
    "    \n",
    "    \n",
    "Once you have run through the notebook once, you may wish to alter the optimizer used in the `train()` function to see how large an effect the choice of optimizer can have.\n",
    "\n",
    "We've highlighted in the comments a number of possible optimizers to use from the [PyTorch Optimizers](https://pytorch.org/docs/stable/optim.html).\n",
    "\n",
    "    \n",
    "You can learn more about different optimization algorithms [here](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6).\n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8353f26",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "### Train our heat equation PINN\n",
    "\n",
    "1. Initialize loss function and optimizer:\n",
    "\n",
    "* criterion = `nn.MSELoss()`\n",
    "* optimizer = `torch.optim.Adam(model.parameters(), lr=learning_rate)`\n",
    "\n",
    "2. Prepare input data:\n",
    "\n",
    "Extract `x` and `t` from `X`.\n",
    "Convert `x`, `t`, and `u` to PyTorch tensors with `requires_grad=True`.\n",
    "\n",
    "4. Training loop:\n",
    "\n",
    "* Loop over the number of iterations (nIter).\n",
    "* Zero the gradients: optimizer.zero_grad().\n",
    "* Predict u and the residual f using the model: u_pred and f_pred.\n",
    "* Compute the loss:\n",
    "    * `loss_PDE`: Mean Squared Error (MSE) between `f_pred` and zero.\n",
    "    * `loss_data`: MSE between `u_tf` and `u_pred`.\n",
    "    * Total loss: loss = loss_PDE + 5*loss_data.\n",
    "* Backpropagate the loss: `loss.backward()`.\n",
    "* Update the model parameters: `optimizer.step()`.\n",
    "\n",
    "5. Print progress:\n",
    "\n",
    "Every 50 iterations, print the current iteration, loss, and elapsed time.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdbd479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nIter, X, u, k,  model, learning_rate=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    start_time = time.time()\n",
    "    x = X[:,0:1]\n",
    "    t = X[:,1:2]\n",
    "    \n",
    "    x_tf = torch.tensor(x, requires_grad=True).float()\n",
    "    t_tf = torch.tensor(t, requires_grad=True).float()\n",
    "    u_tf = torch.tensor(u, requires_grad=True).float()\n",
    "\n",
    "    for it in range(nIter):\n",
    "        optimizer.zero_grad()\n",
    "        u_pred = net_u(x_tf, t_tf, model )\n",
    "        f_pred = net_f(x_tf, t_tf, model,  k)\n",
    "    \n",
    "        loss_PDE = criterion(f_pred, torch.zeros(f_pred.shape))\n",
    "        loss_data = criterion(u_tf, u_pred)\n",
    "        loss = loss_PDE + 5*loss_data\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print\n",
    "        if it % 50 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('It: %d, Loss: %.3e, Time: %.2f' % \n",
    "                          (it, loss.item(), elapsed))\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1678fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "Train_iterations=50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4878e950",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "### Setting Up Tensors in PyTorch\n",
    "\n",
    "In PyTorch, tensors are the primary data structure used for storing and manipulating data. Below is an expanded explanation of setting up tensors, including requiring gradients and setting data types.\n",
    "\n",
    "**Requiring Gradients**:\n",
    "- To enable automatic differentiation, set `requires_grad=True` when creating a tensor. This allows PyTorch to track operations on the tensor and compute gradients during backpropagation.\n",
    "\n",
    "**Data Types**:\n",
    "  - PyTorch supports various data types, such as `float32`, `float64`, `int32`, `int64`, etc.\n",
    "  - The data type can be specified using the `dtype` argument when creating a tensor, as your data types must match when performing operations.\n",
    " \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f419efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_u_train[:,0:1]\n",
    "t = X_u_train[:,1:2]\n",
    "\n",
    "x_tf = torch.tensor(x, requires_grad=True)\n",
    "t_tf = torch.tensor(t, requires_grad=True)\n",
    "u_tf = torch.tensor(u_train, requires_grad=True)\n",
    "\n",
    "u_pred = net_u(x_tf, t_tf, model )\n",
    "f_pred = net_f(x_tf, t_tf, model,k)\n",
    "model = model.float()\n",
    "x_tf = x_tf.float()\n",
    "t_tf = t_tf.float()\n",
    "u_tf = u_tf.float()\n",
    "\n",
    "\n",
    "# Now you can call your train function\n",
    "train(Train_iterations, X_u_train, u_train,k, model, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc849d67",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "# Use trained model to predict from data sample\n",
    "    \n",
    "The function `predict` will predict `u` using the trained model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6547f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x_star, t_star):\n",
    "    \"\"\"\n",
    "    Predicts the solution u and the residual f of the 1D heat equation using the trained PINN model.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The trained Physics-Informed Neural Network model.\n",
    "    x_star (numpy.ndarray): Array of spatial points where predictions are to be made.\n",
    "    t_star (numpy.ndarray): Array of temporal points where predictions are to be made.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - u_star (numpy.ndarray): Predicted solution at the given spatial and temporal points.\n",
    "        - f_star (numpy.ndarray): Predicted residual of the PDE at the given spatial and temporal points.\n",
    "    \"\"\"\n",
    "    # Convert input spatial points to a PyTorch tensor with gradient tracking enabled\n",
    "    x_star_tf = torch.tensor(x_star, requires_grad=True)\n",
    "    # Convert input temporal points to a PyTorch tensor with gradient tracking enabled\n",
    "    t_star_tf = torch.tensor(t_star, requires_grad=True)\n",
    "    # Predict the solution u using the model\n",
    "    u_star = net_u(x_star_tf, t_star_tf, model)\n",
    "    # Predict the residual f of the PDE using the model\n",
    "    f_star = net_f(x_star_tf, t_star_tf, model,k)\n",
    "     \n",
    "    # Detach the predictions from the computation graph and convert to NumPy arrays\n",
    "    return u_star.detach().numpy(), f_star.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086ac50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u_pred, f_pred = predict(model,X_star[:,0:1],X_star[:,1:2])\n",
    "\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2f300b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "# Calculate errors\n",
    "    \n",
    "if you have set the number of training iterations large enough then the errors should be relatively small. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"f_pred mean = \", np.mean(f_pred))\n",
    "print('Error u: %e' % (error_u))\n",
    "print('Percent error u: ',  100*error_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set grid values back to full data set size for plotting\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "X, T = np.meshgrid(x,t) \n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X,T), method='cubic')\n",
    "Error = np.abs(Exact - U_pred)\n",
    "percentError = 100*np.divide(Error, Exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a040f967",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "# Plot exact and predicted $(u,t)$\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "ax.axis('off')\n",
    "\n",
    "print(\"--------- Errors ---------\")\n",
    "print('Percent error u: ',  100*error_u)\n",
    "print(\"--------------------------\")\n",
    "\n",
    "####### Row 0: u(t,x) ##################\n",
    "gs0 = gridspec.GridSpec(3, 2)\n",
    "gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0, hspace=1)\n",
    "\n",
    "########## Prediction ##################\n",
    "ax = plt.subplot(gs0[0, :])\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow',\n",
    "              extent=[t.min(), t.max(), x.min(), x.max()],\n",
    "              origin='lower', aspect='auto', vmin = 0, vmax = 1)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(h, cax=cax)\n",
    "\n",
    "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[2]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[5]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[10]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "ax.legend(frameon=False, loc = 'best')\n",
    "ax.set_title('$u(t,x) - Prediction$', fontsize = 10)\n",
    "\n",
    "########## Exact ##################\n",
    "ax = plt.subplot(gs0[1, :])\n",
    "i = ax.imshow(Exact.T, interpolation='nearest', cmap='rainbow',\n",
    "              extent=[t.min(), t.max(), x.min(), x.max()],\n",
    "              origin='lower', aspect='auto', vmin = 0, vmax = 1)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(i, cax=cax)\n",
    "\n",
    "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[2]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[5]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[10]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "ax.legend(frameon=False, loc = 'best')\n",
    "ax.set_title('$u(t,x)$ - Exact', fontsize = 10)\n",
    "\n",
    "########## Error ##################\n",
    "ax = plt.subplot(gs0[2, :])\n",
    "j = ax.imshow(percentError.T, interpolation='nearest', cmap='rainbow',\n",
    "              extent=[t.min(), t.max(), x.min(), x.max()],\n",
    "              origin='lower', aspect='auto', vmin = 0, vmax = 10)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(j, cax=cax)\n",
    "\n",
    "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[2]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[5]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[10]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "ax.legend(frameon=False, loc = 'best')\n",
    "ax.set_title('$u(t,x)$ - Percent Error', fontsize = 10)\n",
    "\n",
    "####### Row 1: u(t,x) slices ##################\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "ax.axis('off')\n",
    "gs1 = gridspec.GridSpec(1, 3)\n",
    "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[:, 0])\n",
    "ax.plot(x,Exact[2,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[2,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.set_title('$t = ' + str(t[2,0]) + '$', fontsize = 10)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([0,1])\n",
    "\n",
    "ax = plt.subplot(gs1[:, 1])\n",
    "ax.plot(x,Exact[5,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[5,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_title('$t = ' + str(t[5,0]) + '$', fontsize = 10)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "\n",
    "ax = plt.subplot(gs1[:, 2])\n",
    "ax.plot(x,Exact[10,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[10,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_title('$t = ' + str(t[10,0]) + '$', fontsize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075549af",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "**Results**\n",
    "\n",
    "Above are the results of the PINN. The error for recreating the full solution field is $\\approx 10 \\%$, despite using only $N_u = 100$ data points. This shows the power of PINNs to learn from sparse measurements by augmenting the available observational data with knowledge of the underlying physics (i.e. governing equations). \n",
    "\n",
    "The three colour maps show the PINN prediction, the exact solution from the numerical method and the relative error between these two fields. We can see that the errors are largest near $t=0$ and $x=0$, but that overall the agreement is very good.\n",
    "\n",
    "On the colourmap, we can see three vertical white lines, which show the location in time of the three profile plots of $u$ against $x$. The three heat profiles at these times are plotted against the exact solution found using numerical methods. The profiles can be seen to be in very good agreement, but show worse agreement.\n",
    "\n",
    "**Further Work**\n",
    "\n",
    "Congratulations, you have now trained your first physics-informed neural network!\n",
    "\n",
    "This network contains a number of hyper-parameters that could be tuned to give better results. Various hyper-parameters include:\n",
    "- number of data training points N_u,\n",
    "- number of collocation training points N_f,\n",
    "- number of layers in the network,\n",
    "- number of neurons per layer,\n",
    "- weightings for the data and PDE loss terms in the loss function (currently we use loss = loss_PDE + 5*loss_data).\n",
    "\n",
    "It is also possible to use different sampling techniques for training data points. We randomly select $N_u$ data points, but alternative methods could be choosing only boundary points or choosing more points near the $t=0$ boundary. Choosing boundary points for training could help to reduce the errors seen in these regions.\n",
    "\n",
    "Feel free to try out some of these changes if you like!\n",
    "\n",
    "The next part of this notebook looks at the corresponding inverse problem for the 1D heat equation.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffaec48",
   "metadata": {},
   "source": [
    "# 1D Heat Equation Inverse\n",
    "\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "Remembering that in 1D, the heat equation can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial u}{\\partial t} = k \\frac{\\partial^2 u}{\\partial x^2 }\n",
    "\\end{equation}\n",
    "\n",
    "where $k$ is a material parameter called the coefficient of thermal diffusivity. For this notebook, we have solved the above equation numerically on a domain of $x \\in [0,1]$ and $t \\in [0, 0.25]$. Solving this equation numerically gives us a spatiotemporal domain $(x,t)$ and corresponding values of the solution $u$.\n",
    "\n",
    "**Now we will let $k$ be an unknown input parameter in the PINN**. In reality, we know the value of $k$, as we set it when solving the system numerically, but for the sake of this example let's imagine we do not know the value of $k$ when we come to use the PINN. This corresponds to real-world problems where we may have observational data, knowledge of the governing equations, but little information for some input parameters for the system.\n",
    "\n",
    "The network architecture for this example is the same as for the previous [example](#1D-Heat-Equation-Forwards). The only difference is that this time we do not know the value for $k$, and so in each training iteration we do not only updates the network weights and biases, but also the value of $k$. Through training, the network will then optimise the value of $k$ such that it fits with the observed data.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the k value used to generate the data.\n",
    "## We use this to compare to the value found by the PINN.\n",
    "k_exact = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd00c552",
   "metadata": {},
   "source": [
    "# Initialization (inverse problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0914331",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "Once you have run through the notebook once, you may wish to alter any the following:\n",
    "    \n",
    "- number of data training points `N_u`,\n",
    "- number of collocation training points `N_f`,\n",
    "- number of layers in the network `layers`,\n",
    "- number of neurons per layer `layers`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20519b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_u = 100 # number of data points (default 100)\n",
    "N_f = 2000 # collocation points (default 2000)\n",
    "# structure of network: two inputs (x,t) and one output u\n",
    "# 8 fully connected layers with 20 nodes per layer\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11107983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is duplicated from above in case you have been playing with parameters.\n",
    "data = scipy.io.loadmat(\"Data/heatEquation_data.mat\")\n",
    "t = data['t'].flatten()[:,None] # read in t and flatten into column vector\n",
    "x = data['x'].flatten()[:,None] # read in x and flatten into column vector\n",
    "# Exact represents the exact solution to the problem, from the Matlab script provided\n",
    "Exact = np.real(data['usol']).T # Exact has structure of nx times nt\n",
    "\n",
    "# print(\"t = \", t.transpose())\n",
    "# print(\"x = \", x.transpose())\n",
    "print(\"usol shape = \", Exact.shape)\n",
    "\n",
    "# We need to find all the x,t coordinate pairs in the domain\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "# Flatten the coordinate grid into pairs of x,t coordinates\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) # coordinates x,t\n",
    "u_star = Exact.flatten()[:,None]   # corresponding solution value with each coordinate            \n",
    "\n",
    "print(\"X has shape \", X.shape, \", X_star has shape \", X_star.shape)\n",
    "    \n",
    "# Domain bounds (-1,1)\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)  \n",
    "\n",
    "print(\"Lower bounds of x,t: \", lb)\n",
    "print(\"Upper bounds of x,t: \", ub)\n",
    "\n",
    "## train using internal points\n",
    "X_u_train = X_star\n",
    "u_train = u_star\n",
    "\n",
    "## Generate collocation points using Latin Hypercube sampling within the bounds of the spationtemporal coordinates\n",
    "# Generate N_f x,t coordinates within range of upper and lower bounds\n",
    "X_f_train = lb + (ub-lb)*lhs(2, N_f) # the 2 denotes the number of coordinates we have - x,t \n",
    "\n",
    "## In addition, we add the X_u_train coordinats from the boundaries to the X_f coordinate set\n",
    "X_f_train = np.vstack((X_f_train, X_u_train)) # stack up all training x,t coordinates for u and f \n",
    "\n",
    "## We downsample the boundary data to leave N_u randomly distributed points\n",
    "## This makes the training more difficult - \n",
    "## if we used all the points then there is not much for the network to do!\n",
    "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "X_u_train = X_star[idx,:]\n",
    "u_train = u_star[idx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb252d1d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "Now we will use all the same fuctions as before, except we will modify `k` and the train function to handle a variable `k` value.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90247095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data using true k\n",
    "k = k_exact\n",
    "k = torch.tensor(float(k), requires_grad=True)\n",
    "model = NeuralNet(layers,lb,ub)\n",
    "x = X_u_train[:,0:1]\n",
    "t = X_u_train[:,1:2]\n",
    "\n",
    "\n",
    "x_t = torch.tensor(x, requires_grad=True)\n",
    "t_t = torch.tensor(t, requires_grad=True)\n",
    "u_t = torch.tensor(u_train, requires_grad=True)\n",
    "\n",
    "u_pred = net_u(x_t, t_t, model)\n",
    "f_pred = net_f(x_t, t_t, model,k)\n",
    "model = model.float()\n",
    "x_t = x_t.float()\n",
    "t_t = t_t.float()\n",
    "u_t = u_t.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28bda7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "**Training might take a while depending on value of Train_iterations**\n",
    "\n",
    "If you set Train_iterations too low the end results will be poor. 50000 was used to achieve excellent results. \n",
    "\n",
    "* If you are using a machine with GPUs please set `Train_iterations` to 50000 and this will run quickly.\n",
    "* If you are using a well spec'ed laptop/computer, you can leave this setting `Train_iterations=50000` but it will take up to 10 mins.\n",
    "* If you are using a low spec'ed laptop/computer or cannot leave the code running, `Train_iterations=20000` is the recommended value (this solution may not be accurate).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "Train_iterations=50000\n",
    "train(Train_iterations, X_u_train, u_train,k, model, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred, f_pred = predict(model,X_star[:,0:1],X_star[:,1:2])\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2)/np.linalg.norm(u_star, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de55ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"f_pred mean = \", np.mean(f_pred))\n",
    "print('Error u: %e' % (error_u))\n",
    "print('Percent error u: ',  100*error_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281887c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set grid values back to full data set size for plotting\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "X, T = np.meshgrid(x,t) \n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X,T), method='cubic')\n",
    "Error = np.abs(Exact - U_pred)\n",
    "percentError = 100*np.divide(Error, Exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "ax.axis('off')\n",
    "\n",
    "print(\"--------- Errors ---------\")\n",
    "print('Percent error u: ',  100*error_u)\n",
    "print(\"--------------------------\")\n",
    "\n",
    "####### Row 0: u(t,x) ##################\n",
    "gs0 = gridspec.GridSpec(3, 2)\n",
    "gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0, hspace=1)\n",
    "\n",
    "########## Prediction ##################\n",
    "ax = plt.subplot(gs0[0, :])\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow',\n",
    "              extent=[t.min(), t.max(), x.min(), x.max()],\n",
    "              origin='lower', aspect='auto', vmin = 0, vmax = 1)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(h, cax=cax)\n",
    "\n",
    "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[2]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[5]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[10]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "ax.legend(frameon=False, loc = 'best')\n",
    "ax.set_title('$u(t,x) - Prediction$', fontsize = 10)\n",
    "\n",
    "########## Exact ##################\n",
    "ax = plt.subplot(gs0[1, :])\n",
    "i = ax.imshow(Exact.T, interpolation='nearest', cmap='rainbow',\n",
    "              extent=[t.min(), t.max(), x.min(), x.max()],\n",
    "              origin='lower', aspect='auto', vmin = 0, vmax = 1)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(i, cax=cax)\n",
    "\n",
    "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[2]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[5]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[10]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "ax.legend(frameon=False, loc = 'best')\n",
    "ax.set_title('$u(t,x)$ - Exact', fontsize = 10)\n",
    "\n",
    "########## Error ##################\n",
    "ax = plt.subplot(gs0[2, :])\n",
    "j = ax.imshow(percentError.T, interpolation='nearest', cmap='rainbow',\n",
    "              extent=[t.min(), t.max(), x.min(), x.max()],\n",
    "              origin='lower', aspect='auto', vmin = 0, vmax = 10)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(j, cax=cax)\n",
    "\n",
    "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[2]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[5]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[10]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "ax.legend(frameon=False, loc = 'best')\n",
    "ax.set_title('$u(t,x)$ - Percent Error', fontsize = 10)\n",
    "\n",
    "####### Row 1: u(t,x) slices ##################\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "ax.axis('off')\n",
    "gs1 = gridspec.GridSpec(1, 3)\n",
    "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[:, 0])\n",
    "ax.plot(x,Exact[2,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[2,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.set_title('$t = ' + str(t[2,0]) + '$', fontsize = 10)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([0,1])\n",
    "\n",
    "ax = plt.subplot(gs1[:, 1])\n",
    "ax.plot(x,Exact[5,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[5,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_title('$t = ' + str(t[5,0]) + '$', fontsize = 10)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "\n",
    "ax = plt.subplot(gs1[:, 2])\n",
    "ax.plot(x,Exact[10,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[10,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_title('$t = ' + str(t[10,0]) + '$', fontsize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca918c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "**Results**\n",
    "\n",
    "Above are the results of the PINN. The error for recreating the full solution field is $\\approx 5 \\%$, despite using only $N_u = 100$ data points. This shows the power of PINNs to learn solution fields from sparse measurements, even when some of the input parameters are unknown.\n",
    "\n",
    "The three colour maps show the PINN prediction, the exact solution from the numerical method and the relative error between these two fields. We can see that the errors are largest near $t=0$ and $x=0$, but that overall the agreement is very good.\n",
    "\n",
    "On the colour map, we can see three vertical white lines, which show the location in time of the three profile plots of $u$ against $x$. The three heat profiles at these times are plotted against the exact solution found using numerical methods. The profiles can be seen to be in very good agreement, but show worse agreement.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84c8c3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffcc ; padding: 10px;\">\n",
    "\n",
    "**Further Work**\n",
    "\n",
    "Congratulations, you have now trained your second physics-informed neural network!\n",
    "\n",
    "This network contains a number of hyper-parameters that could be tuned to give better results. Various hyper-parameters include:\n",
    "- number of data training points N_u,\n",
    "- number of collocation training points N_f,\n",
    "- number of layers in the network,\n",
    "- number of neurons per layer,\n",
    "- weightings for the data and PDE loss terms in the loss function (currently we use loss = loss_PDE + 5*loss_data)\n",
    "- initialisation value for k,\n",
    "- optimisation.\n",
    "\n",
    "It is also possible to use different sampling techniques for training data points. We randomly select $N_u$ data points, but alternative methods could be choosing only boundary points or choosing more points near the $t=0$ boundary.\n",
    "\n",
    "    \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ca4e16",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "# Next Steps\n",
    "    \n",
    "Next we move on to a more complex example using the Navier Stokes Equation in the next notebook.\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PINN_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
